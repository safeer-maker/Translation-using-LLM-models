{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq , AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors=\"pt\")\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('kde4', 'en-fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['118328', '75379', '83158', '14769', '12622'],\n",
       " 'translation': [{'en': 'Show Pathfinder Lander Image',\n",
       "   'fr': \"Afficher l'image de Pathfinder LanderImage/ info menu item (should be translated)\"},\n",
       "  {'en': 'Publisher', 'fr': 'Éditeur'},\n",
       "  {'en': 'Remove entry', 'fr': \"Supprimer l' entrée\"},\n",
       "  {'en': 'Application Preference Order determines which applications will be associated with the specified & MIME; type.',\n",
       "   'fr': 'Applications par ordre de préférence & #160;: détermine les associations qui sont associées à ce type & MIME;.'},\n",
       "  {'en': 'CSS Validator', 'fr': 'Validateur CSS'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train', Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 189155\n",
      "}))\n",
      "('test', Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 21018\n",
      "}))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'] = dataset.pop(\"test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['118328', '75379'],\n",
       " 'translation': [{'en': 'Show Pathfinder Lander Image',\n",
       "   'fr': \"Afficher l'image de Pathfinder LanderImage/ info menu item (should be translated)\"},\n",
       "  {'en': 'Publisher', 'fr': 'Éditeur'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = dataset['train']\n",
    "train_ds[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Show Pathfinder Lander Image',\n",
       " \"Afficher l'image de Pathfinder LanderImage/ info menu item (should be translated)\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds['translation'][0]['en'], train_ds['translation'][0]['fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:  Publisher\n",
      "french:  Éditeur\n",
      "token_en:  {'input_ids': [28792, 0], 'attention_mask': [1, 1], 'labels': [17963, 955, 227, 705, 0]}\n",
      "detokenizing :  ['▁Publisher', '</s>']  fr  ['▁É', 'di', 'te', 'ur', '</s>']\n",
      "token_en_test:  {'input_ids': [28792, 0], 'attention_mask': [1, 1]}\n",
      "token_fr_test:  {'input_ids': [17963, 25137, 5488, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "detoken en:  ['▁Publisher', '</s>']\n",
      "detoken fr:  ['▁É', 'dit', 'eur', '</s>']\n",
      "token_fr:  {'input_ids': [17963, 25137, 5488, 0], 'attention_mask': [1, 1, 1, 1], 'labels': [23917, 6, 1279, 0]}\n",
      "detokenizing :  ['▁É', 'dit', 'eur', '</s>']  fr  ['▁Publi', 's', 'her', '</s>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "english = train_ds[1]['translation']['en']\n",
    "french = train_ds[1]['translation']['fr']\n",
    "\n",
    "print (\"english: \", english), print (\"french: \",french)\n",
    "\n",
    "token_en = tokenizer(english, text_target=french)\n",
    "print (\"token_en: \", token_en)\n",
    "print (\"detokenizing : \", tokenizer.convert_ids_to_tokens(token_en[\"input_ids\"]), \" fr \", tokenizer.convert_ids_to_tokens(token_en[\"labels\"]) )\n",
    "\n",
    "token_en_test = tokenizer(english)\n",
    "token_fr_test = tokenizer(french)\n",
    "\n",
    "print (\"token_en_test: \", token_en_test)\n",
    "print (\"token_fr_test: \", token_fr_test)\n",
    "\n",
    "print(\"detoken en: \", tokenizer.convert_ids_to_tokens(token_en_test[\"input_ids\"]))\n",
    "print(\"detoken fr: \", tokenizer.convert_ids_to_tokens(token_fr_test[\"input_ids\"]))\n",
    "\n",
    "token_fr = tokenizer(french , text_target= english)\n",
    "print (\"token_fr: \", token_fr)\n",
    "print (\"detokenizing : \", tokenizer.convert_ids_to_tokens(token_fr[\"input_ids\"]), \" fr \", tokenizer.convert_ids_to_tokens(token_fr[\"labels\"]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [28792, 17963, 25137, 5488, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_ds[1]['translation']['en'], train_ds[1]['translation']['fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1482, 16449, 34824, 545, 45, 6910, 0], [28792, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1]], 'labels': [[20586, 2585, 4042, 1181, 22, 685, 1056, 230, 16449, 11224, 45, 545, 45, 4166, 1056, 75, 3058, 3099, 3035, 20, 6, 2277, 10382, 43, 3388, 6860, 108, 27, 0], [17963, 955, 227, 705, 0]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    english = [ex['en'] for ex in examples['translation']]\n",
    "    french = [ex['fr'] for ex in examples['translation']]\n",
    "    return tokenizer(english, text_target=french, max_length=128, truncation=True)\n",
    "\n",
    "tokenize_function (train_ds[:2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:  Publisher\n",
      "french:  Éditeur\n",
      "token_en:  {'input_ids': [19816, 1], 'attention_mask': [1, 1], 'labels': [7983, 10700, 450, 1]}\n",
      "detokenizing :  ['▁Publisher', '</s>']  fr  ['▁É', 'dite', 'ur', '</s>']\n",
      "token_en_test:  {'input_ids': [19816, 1], 'attention_mask': [1, 1]}\n",
      "token_fr_test:  {'input_ids': [7983, 10700, 450, 1], 'attention_mask': [1, 1, 1, 1]}\n",
      "detoken en:  ['▁Publisher', '</s>']\n",
      "detoken fr:  ['▁É', 'dite', 'ur', '</s>']\n",
      "token_fr:  {'input_ids': [7983, 10700, 450, 1], 'attention_mask': [1, 1, 1, 1], 'labels': [19816, 1]}\n",
      "detokenizing :  ['▁É', 'dite', 'ur', '</s>']  fr  ['▁Publisher', '</s>']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 't5-small'\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(checkpoint, return_tensors=\"pt\")\n",
    "\n",
    "english = train_ds[1]['translation']['en']\n",
    "french = train_ds[1]['translation']['fr']\n",
    "\n",
    "print (\"english: \", english), print (\"french: \",french)\n",
    "\n",
    "token_en = tokenizer1(english, text_target=french)\n",
    "print (\"token_en: \", token_en)\n",
    "print (\"detokenizing : \", tokenizer1.convert_ids_to_tokens(token_en[\"input_ids\"]), \" fr \", tokenizer1.convert_ids_to_tokens(token_en[\"labels\"]) )\n",
    "\n",
    "token_en_test = tokenizer1(english)\n",
    "token_fr_test = tokenizer1(french)\n",
    "\n",
    "print (\"token_en_test: \", token_en_test)\n",
    "print (\"token_fr_test: \", token_fr_test)\n",
    "\n",
    "print(\"detoken en: \", tokenizer1.convert_ids_to_tokens(token_en_test[\"input_ids\"]))\n",
    "print(\"detoken fr: \", tokenizer1.convert_ids_to_tokens(token_fr_test[\"input_ids\"]))\n",
    "\n",
    "token_fr = tokenizer1(french , text_target= english)\n",
    "print (\"token_fr: \", token_fr)\n",
    "print (\"detokenizing : \", tokenizer1.convert_ids_to_tokens(token_fr[\"input_ids\"]), \" fr \", tokenizer1.convert_ids_to_tokens(token_fr[\"labels\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns= dataset['train'].column_names,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 189155\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token = train_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns= train_ds.column_names,\n",
    ")\n",
    "\n",
    "train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189155\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 21018\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint) \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = DataCollatorForSeq2Seq (tokenizer=tokenizer, model=model ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(1, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  [{'input_ids': [28792, 0], 'attention_mask': [1, 1], 'labels': [17963, 955, 227, 705, 0]}, {'input_ids': [9122, 2995, 0], 'attention_mask': [1, 1, 1], 'labels': [124, 641, 16205, 45, 1181, 22, 1170, 7313, 18, 0]}]\n",
      "batch1 :  {'input_ids': tensor([[28792,     0, 58100],\n",
      "        [ 9122,  2995,     0]]), 'attention_mask': tensor([[1, 1, 0],\n",
      "        [1, 1, 1]]), 'labels': tensor([[17963,   955,   227,   705,     0,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  124,   641, 16205,    45,  1181,    22,  1170,  7313,    18,     0]]), 'decoder_input_ids': tensor([[    0, 17963,   955,   227,   705,     0,     0,     0,     0,     0],\n",
      "        [    0,   124,   641, 16205,    45,  1181,    22,  1170,  7313,    18]])}\n"
     ]
    }
   ],
   "source": [
    "batch = [train_token[i] for i in range (1,3) ]\n",
    "print (\"batch: \", batch)\n",
    "\n",
    "batch1 = data_collector (batch) \n",
    "print(\"batch1 : \", batch1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']),\n",
       " tensor([[    0, 17963,   955,   227,   705,     0,     0,     0,     0,     0],\n",
       "         [    0,   124,   641, 16205,    45,  1181,    22,  1170,  7313,    18]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1.keys(), batch1['decoder_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [28792, 0],\n",
       " 'attention_mask': [1, 1],\n",
       " 'labels': [17963, 955, 227, 705, 0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    #fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\Translation-using-LLM-models\\Translation.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Seq2SeqTrainer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     args,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mtokenized_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collector,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     43\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     44\u001b[0m     model: Union[\u001b[39m\"\u001b[39m\u001b[39mPreTrainedModel\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     55\u001b[0m ):\n\u001b[1;32m---> 56\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     57\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     58\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m     59\u001b[0m         data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m     60\u001b[0m         train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[0;32m     61\u001b[0m         eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[0;32m     62\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     63\u001b[0m         model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[0;32m     64\u001b[0m         compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[0;32m     65\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m     66\u001b[0m         optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[0;32m     67\u001b[0m         preprocess_logits_for_metrics\u001b[39m=\u001b[39;49mpreprocess_logits_for_metrics,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m     \u001b[39m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[39m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:331\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[0;32m    330\u001b[0m \u001b[39m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m enable_full_determinism(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mseed) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mfull_determinism \u001b[39melse\u001b[39;00m set_seed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mseed)\n\u001b[0;32m    332\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer_utils.py:94\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     92\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 94\u001b[0m     torch\u001b[39m.\u001b[39;49mmanual_seed(seed)\n\u001b[0;32m     95\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m     96\u001b[0m     \u001b[39m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 40\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmanual_seed_all(seed)\n\u001b[0;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmps\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    110\u001b[0m         default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    111\u001b[0m         default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 113\u001b[0m _lazy_call(cb, seed_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\__init__.py:183\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_call\u001b[39m(callable, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 183\u001b[0m         callable()\n\u001b[0;32m    184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m         \u001b[39m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m         \u001b[39m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    187\u001b[0m         \u001b[39m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[39mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m    110\u001b[0m     default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 111\u001b[0m     default_generator\u001b[39m.\u001b[39;49mmanual_seed(seed)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collector,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AcceleratorState' object has no attribute 'distributed_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Z004RJZU\\Documents\\LLM_Practice\\Translation-using-LLM-models\\Translation.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Z004RJZU/Documents/LLM_Practice/Translation-using-LLM-models/Translation.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1551\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrently training with a batch size of: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1552\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 1553\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[0;32m   1555\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:854\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    851\u001b[0m     dataloader_params[\u001b[39m\"\u001b[39m\u001b[39mdrop_last\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_drop_last\n\u001b[0;32m    852\u001b[0m     dataloader_params[\u001b[39m\"\u001b[39m\u001b[39mworker_init_fn\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m seed_worker\n\u001b[1;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mprepare(DataLoader(train_dataset, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdataloader_params))\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\accelerate\\accelerator.py:1206\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[1;34m(self, device_placement, *args)\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1197\u001b[0m         \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[0;32m   1198\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_device_map(obj)\n\u001b[0;32m   1199\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mNO\n\u001b[0;32m   1200\u001b[0m     ):\n\u001b[0;32m   1201\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1202\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt train a model that has been loaded with `device_map=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` in any distributed mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1203\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[39m{{\u001b[39m\u001b[39mmyscript.py}}`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1204\u001b[0m         )\n\u001b[1;32m-> 1206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mFSDP:\n\u001b[0;32m   1207\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfsdp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfully_sharded_data_parallel\u001b[39;00m \u001b[39mimport\u001b[39;00m FullyShardedDataParallel \u001b[39mas\u001b[39;00m FSDP\n\u001b[0;32m   1209\u001b[0m     model_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\accelerate\\accelerator.py:474\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistributed_type\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 474\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mdistributed_type\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AcceleratorState' object has no attribute 'distributed_type'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i have trained the model with small dataset as y personal system will take ablout a day to finish training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get our model that we have trained and get some translation done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(59514, 512, padding_idx=59513)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"marian-finetuned-kde4-en-to-fr\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I am a student, Lets Translate it into French\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"marian-finetuned-kde4-en-to-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   47,  1010,    15,  6548,     2,  2618,     9, 35742,    61,   208,\n",
       "          1109,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sample = tokenizer(sample_text, return_tensors=\"pt\", )\n",
    "token_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   131,   558,    34, 16308,     2,    19,    75,     9,  4999,\n",
       "         10741,    61,    18,  1109,     0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out = model.generate(**token_sample)\n",
    "model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([59513,   131,   558,    34, 16308,     2,    19,    75,     9,  4999,\n",
       "        10741,    61,    18,  1109,     0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Je suis un étudiant, lets Translate it in French'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model_out.squeeze(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Z004RJZU\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment allez -vous & #160;?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model from a local path\n",
    "model_path = \"marian-finetuned-kde4-en-to-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "# Define a function for translation\n",
    "def translate_to_french(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "    translation_ids = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example translation\n",
    "text_to_translate = \"Hello, how are you?\"\n",
    "translated_text = translate_to_french(text_to_translate)\n",
    "print(translated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
